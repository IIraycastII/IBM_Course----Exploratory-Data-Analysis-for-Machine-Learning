Lecture-30
Bayesian representation of Hypothesis testing example

Concept Overview:
This section explains how Bayesian hypothesis testing builds upon the coin-toss example by incorporating prior beliefs (priors) into our decision-making process. Unlike the frequentist approach, Bayesian inference updates our belief about which hypothesis is more likely after seeing the data, producing a posterior probability for each hypothesis.

Setup:

We have two coins:

Coin 1: 
ğ‘ƒ(heads)=0.5
P(heads)=0.5

Coin 2: 
ğ‘ƒ(heads)=0.7
P(heads)=0.7

One coin is chosen at random (we donâ€™t know which) and tossed 10 times.

Based on the number of heads observed, we estimate which coin is more likely to have been chosen.

Step 1: Define Priors

Since the coin was chosen randomly, we start with equal priors:

ğ‘ƒ(ğ»1)=0.5 and ğ‘ƒ(ğ»2)=0.5
P(H1)=0.5 and P(H2)=0.5

meaning both coins are equally likely before observing any data.

However, if we were drawing from real-world coins (where most are fair), our prior for the fair coin could be much higher â€” for example,
ğ‘ƒ(ğ»1)=0.99,
ğ‘ƒ(ğ»2)=0.01
P(H1)=0.99,P(H2)=0.01

making it harder to conclude bias unless the data strongly supports it.

Step 2: Apply Bayesâ€™ Rule

We use Bayesâ€™ theorem to update our belief about each hypothesis after observing the data (e.g., 3 heads in 10 tosses):

ğ‘ƒ(ğ»ğ‘–âˆ£data)=ğ‘ƒ(dataâˆ£ğ»ğ‘–)â‹…ğ‘ƒ(ğ»ğ‘–)ğ‘ƒ(data)
P(Hiâˆ£data)=P(data)P(dataâˆ£Hi)â‹…P(Hi)â€‹

ğ‘ƒ(dataâˆ£ğ»ğ‘–)
P(dataâˆ£Hi) = likelihood of observing the data under hypothesis ğ»ğ‘–Hi	â€‹

ğ‘ƒ(ğ»ğ‘–)P(Hi) = prior belief before seeing the data

ğ‘ƒ(ğ»ğ‘–âˆ£data)P(Hiâ€‹âˆ£data) = posterior probability, our updated belief

The denominator 
ğ‘ƒ(data)
P(data) just normalizes probabilities â€” itâ€™s not essential for comparing hypotheses.

Step 3: Likelihood Ratio and Updating

The likelihood ratio compares how likely the observed data is under each hypothesis:

Likelihood Ratio=ğ‘ƒ(dataâˆ£ğ»1)ğ‘ƒ(dataâˆ£ğ»2)
Likelihood Ratio=P(dataâˆ£H2)P(dataâˆ£H1)
	â€‹


In Bayesian analysis, we multiply the priors by this likelihood ratio to get the posterior odds:

Posterior Odds
=
Prior Odds
Ã—
Likelihood Ratio
Posterior Odds=Prior OddsÃ—Likelihood Ratio

If the priors are equal (50/50), they cancel out â€” so the decision depends solely on the data.
If priors are unequal (e.g., 0.99 vs 0.01), much stronger evidence is needed to favor the less probable hypothesis.

Key Idea:

Likelihood Ratio â†’ comes from data.

Priors â†’ come from prior belief or experience.

Posteriors â†’ updated probabilities after combining both.
