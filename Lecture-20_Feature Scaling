Lecture 21
Feature Scaling

Learning Goals

In this lecture, weâ€™ll learn how to adjust the scale of continuous variables so that they can be compared meaningfully and treated fairly by machine learning algorithms. Weâ€™ll cover:

Why scaling is necessary for features with different magnitudes

How unscaled data can distort distance-based algorithms like K-Nearest Neighbors (KNN)

The three main scaling techniques: Standard Scaling, Min-Max Scaling, and Robust Scaling

When and why to use each scaling method

Why Feature Scaling Matters

In real-world datasets, different features often exist on different scales.
For example:

Product price: ranges between 0â€“10

Number of stores: ranges between 10,000â€“50,000

Without scaling, features with larger numerical ranges dominate others, leading models to assign disproportionate importance to them.

Example: K-Nearest Neighbors (KNN)

KNN determines a pointâ€™s label based on its nearest neighbors in feature space.

Imagine two features:

Age (measured in seconds)

Number of surgeries

If age is in seconds, one year = 31.5 million units.
So a 1-year age difference (31.5M units) will dwarf the difference between patients who had 1 vs. 10 surgeries (only 9 units apart).

As a result, KNN would group patients based mostly on age, ignoring the number of surgeries, even if surgeries are more predictive of risk.

Feature scaling corrects this by putting both variables on a comparable scale, ensuring the algorithm gives both features fair weight.

Types of Feature Scaling
1. Standard Scaling (Z-score Normalization)

Formula:
ğ‘¥â€²=ğ‘¥âˆ’mean(ğ‘¥)std(ğ‘¥)
xâ€²=std(x)xâˆ’mean(x)	â€‹

Centers data around 0 with a standard deviation of 1

Maintains original data distribution shape

Sensitive to outliers, since mean and standard deviation can be affected by extreme values

Commonly used in models assuming normally distributed features (e.g., Linear Regression, Logistic Regression, SVMs)

2. Min-Max Scaling (Normalization)

Formula:

ğ‘¥â€²=ğ‘¥âˆ’min(ğ‘¥)max(ğ‘¥)âˆ’min(ğ‘¥)
xâ€²=max(x)âˆ’min(x)xâˆ’min(x)	â€‹

Rescales all values into a [0, 1] range

Useful when you need bounded feature values, such as for neural networks

Highly sensitive to outliers â€” one extreme value can compress the range of all other values

Example:
If min = 3, max = 100, and x = 40:

ğ‘¥â€²=40âˆ’3100âˆ’3=3797â‰ˆ0.38
xâ€²=100âˆ’340âˆ’3â€‹=9737â‰ˆ0.38

If one data point is 10,000, all other points (0â€“10) would be squeezed near 0.

3. Robust Scaling

Formula:

ğ‘¥â€²=ğ‘¥âˆ’median(ğ‘¥)IQR
xâ€²=IQR xâˆ’median(x)
	â€‹


where IQR = Interquartile Range (Q3 âˆ’ Q1)

Uses median and IQR instead of mean and standard deviation

Resistant to outliers â€” ideal for skewed or heavy-tailed distributions

Does not guarantee values fall between 0 and 1

Choosing the Right Scaling Method
Method	Best For	Sensitive to Outliers	Output Range
Standard Scaling	Normally distributed data	Yes	No fixed range
Min-Max Scaling	Bounded, non-outlier data	Yes	[0, 1]
Robust Scaling	Data with outliers or skew	No	Variable
