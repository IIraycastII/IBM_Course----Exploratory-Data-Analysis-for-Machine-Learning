Lecture 13
Handling Missing Values and Outliers

Learning Goals

In this lecture, we’ll explore best practices for handling missing values and outliers in datasets. We’ll cover:

Why missing data causes issues in machine learning

Common strategies for handling missing values (removal, imputation, masking)

The impact of outliers on models

How to detect and manage outliers using plots and statistical methods

Handling Missing Data

Machine learning models cannot process blank or null values — every feature and label must contain meaningful information. Therefore, we must decide how to address missing data.

1. Remove Missing Data

Method: Remove entire rows or columns containing missing values.

Pros:

Quick and simple method.

No assumptions or estimations required.

Cons:

Risk of losing too much data.

May introduce bias if certain types of data are systematically missing.

Best when only a small fraction of rows are incomplete.

2. Impute Missing Data

Method: Replace missing values with estimated ones — typically the mean, median, or more complex statistical estimates.

Pros:

Retains all observations.

Prevents data loss from deletion.

Cons:

Introduces uncertainty — estimated values may not represent true data.

Best when missingness is moderate and data is relatively stable.

3. Mask Missing Data

Method: Treat missing values as their own category.

Example: In a survey, a blank final response might indicate that the participant hung up — useful information itself.

Pros:

Keeps all data.

Can capture patterns related to missingness.

Cons:

Assumes all missing values are similar in meaning.

Adds complexity and potential noise.

Best when missingness carries meaningful information (e.g., behavioral data).

Handling Outliers

An outlier is a data point that deviates significantly from most other observations. Outliers can distort averages, affect model training, and lead to misleading predictions.

Example

If weekly sales typically range between 10–50, but one week records 3,000, that single outlier can skew the average drastically upward — biasing predictions.

However, not all outliers are bad — they may reveal rare but important events (e.g., a sudden spike in sales due to a marketing campaign). Always investigate before removal.

Detecting Outliers

Outliers can be identified through visualization or statistical analysis.

1. Visual Methods

Histograms & Density Plots: Show distribution and highlight unusual values.

Box Plots: Visualize the interquartile range (IQR), median, and potential outliers.

Example using Seaborn:

import seaborn as sns

sns.displot(data, bins=20)       # Histogram
sns.boxplot(x=data['feature'])   # Box plot

2. Statistical Methods (IQR Rule)

Use the Interquartile Range (IQR) to define normal value boundaries.

import numpy as np

q25, q50, q75 = np.percentile(data['feature'], [25, 50, 75])
iqr = q75 - q25
min_val = q25 - 1.5 * iqr
max_val = q75 + 1.5 * iqr

# Identify outliers
outliers = [x for x in data['feature'] if x < min_val or x > max_val]


Values outside [min_val, max_val] are considered outliers.

3. Residual Analysis

Outliers can also be identified using residuals (differences between predicted and actual values).
Standardized, deleted, or studentized residuals help detect unusually high errors in model predictions.

Key Takeaways

Always inspect your dataset before removing or imputing values.

The best approach for missing data depends on its amount, pattern, and context.

Outliers should be analyzed, not blindly removed — they may hold key insights.
